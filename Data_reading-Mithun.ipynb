{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4552e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "acbb4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_train_pairs.csv',encoding='utf-8')\n",
    "# df_train_trans = pd.read_csv('data_final_0711/Exp2/Hindi/bengaliTranstoHindi.csv',encoding='utf-8')\n",
    "# df_val = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_val_pairs.csv', encoding='utf-8')\n",
    "# df_test = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_test_pairs.csv', encoding='utf-8')\n",
    "\n",
    "df_train = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_train_pairs.csv',encoding='utf-8',lineterminator='\\n')\n",
    "df_train_trans = pd.read_csv('data_final_0711/Exp2/Bengali/hindiTranstoBengali.csv',encoding='utf-8')\n",
    "df_val = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_val_pairs.csv', encoding='utf-8')\n",
    "df_test = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_test_pairs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640deb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def util(df_train, save_file_name):\n",
    "    df_train = df_train[['texts','counterSpeechs']]\n",
    "    df_train['prefix'] = 'counterspeech'\n",
    "    first_column = df_train.pop('prefix')\n",
    "    df_train.insert(0, 'prefix', first_column)\n",
    "    df_train = df_train.rename(columns={'texts': 'input_text', 'counterSpeechs': 'target_text'})\n",
    "    df_train.to_csv('data_final/Exp2/Bengali/'+save_file_name,encoding='utf-8',index=False)\n",
    "#     print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d5d02727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trans = df_train_trans[['hatespeech_trans','counterspeech_trans']]\n",
    "df_train_trans = df_train_trans.rename(columns = {'hatespeech_trans':'texts', 'counterspeech_trans':'counterSpeechs'})\n",
    "df_combined = df_train.append(df_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b47a2828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-79044556ebc6>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['prefix'] = 'counterspeech'\n"
     ]
    }
   ],
   "source": [
    "# util(df_combined,'hindi_train_pairs.csv')\n",
    "# util(df_val,'hindi_val_pairs.csv')\n",
    "# util(df_test,'hindi_test_pairs.csv')\n",
    "\n",
    "util(df_combined,'bengali_train_pairs.csv')\n",
    "util(df_val,'bengali_val_pairs.csv')\n",
    "util(df_test,'bengali_test_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794efe13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9f25755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>তাসলিমা আমার বাড়ির কুত্তা তোক চুইদা তোর ভোদা ...</td>\n",
       "      <td>আপনার মুখের ভাষা খুবই জঘন্য। কোনো ব্যক্তিকে অভ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>তাসলিমা আমার বাড়ির কুত্তা তোক চুইদা তোর ভোদা ...</td>\n",
       "      <td>লেখক তাসলিমার জন্য আপনার এই মন্তব্য বুঝিয়ে দিছ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>শালী বাবাচুদি খানকি একটা</td>\n",
       "      <td>আপনি গালিগালির প্রদর্শন করছেন বলে মনে হয়। চারট...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>শালী বাবাচুদি খানকি একটা</td>\n",
       "      <td>আপনি যদি নারীদের ঘৃণা করতে থাকেন তাহলে সমাজ আপ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>আমরা চোদনে expart ! আমি কেমন চুদি তোর মাকে জিঞ...</td>\n",
       "      <td>আপনি কিসে এক্সপার্ট কেউ জানতে চাইনি। অভদ্র মন্...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>সত্যি  আমাদের  মন মানসিকতায় এখনও অনেক পিছিয়ে...</td>\n",
       "      <td>আপনি যে কথাটি বুঝতে পেরেছেন এটাই অনেক ভালো, সব...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>আল্লাহ নারীদের দুর্বল করে বানিয়েছেন,  তাদের উ...</td>\n",
       "      <td>পুরুষরা যখন নারীদের তুলনায় বেশি শক্তিশালী, তখন...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>আল্লাহ নারীদের দুর্বল করে বানিয়েছেন,  তাদের উ...</td>\n",
       "      <td>আল্লাহ নারী পুরুষ সকলকেই সমান শক্তি দিয়ে বানিয়...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>আচ্ছা তুই এই ব্যক্তির উপর এত রাগ কেন? সে কি তো...</td>\n",
       "      <td>ওই ব্যক্তির হয়ে কথা বলতে গিয়ে আপনিও তো গালিগাল...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>আচ্ছা তুই এই ব্যক্তির উপর এত রাগ কেন? সে কি তো...</td>\n",
       "      <td>আপনি কাউকে কোনো প্রশ্ন করতে গেলে ভালো ভাবেই কর...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1434 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             prefix                                         input_text  \\\n",
       "0     counterspeech  তাসলিমা আমার বাড়ির কুত্তা তোক চুইদা তোর ভোদা ...   \n",
       "1     counterspeech  তাসলিমা আমার বাড়ির কুত্তা তোক চুইদা তোর ভোদা ...   \n",
       "2     counterspeech                           শালী বাবাচুদি খানকি একটা   \n",
       "3     counterspeech                           শালী বাবাচুদি খানকি একটা   \n",
       "4     counterspeech  আমরা চোদনে expart ! আমি কেমন চুদি তোর মাকে জিঞ...   \n",
       "...             ...                                                ...   \n",
       "1429  counterspeech  সত্যি  আমাদের  মন মানসিকতায় এখনও অনেক পিছিয়ে...   \n",
       "1430  counterspeech  আল্লাহ নারীদের দুর্বল করে বানিয়েছেন,  তাদের উ...   \n",
       "1431  counterspeech  আল্লাহ নারীদের দুর্বল করে বানিয়েছেন,  তাদের উ...   \n",
       "1432  counterspeech  আচ্ছা তুই এই ব্যক্তির উপর এত রাগ কেন? সে কি তো...   \n",
       "1433  counterspeech  আচ্ছা তুই এই ব্যক্তির উপর এত রাগ কেন? সে কি তো...   \n",
       "\n",
       "                                            target_text  \n",
       "0     আপনার মুখের ভাষা খুবই জঘন্য। কোনো ব্যক্তিকে অভ...  \n",
       "1     লেখক তাসলিমার জন্য আপনার এই মন্তব্য বুঝিয়ে দিছ...  \n",
       "2     আপনি গালিগালির প্রদর্শন করছেন বলে মনে হয়। চারট...  \n",
       "3     আপনি যদি নারীদের ঘৃণা করতে থাকেন তাহলে সমাজ আপ...  \n",
       "4     আপনি কিসে এক্সপার্ট কেউ জানতে চাইনি। অভদ্র মন্...  \n",
       "...                                                 ...  \n",
       "1429  আপনি যে কথাটি বুঝতে পেরেছেন এটাই অনেক ভালো, সব...  \n",
       "1430  পুরুষরা যখন নারীদের তুলনায় বেশি শক্তিশালী, তখন...  \n",
       "1431  আল্লাহ নারী পুরুষ সকলকেই সমান শক্তি দিয়ে বানিয়...  \n",
       "1432  ওই ব্যক্তির হয়ে কথা বলতে গিয়ে আপনিও তো গালিগাল...  \n",
       "1433  আপনি কাউকে কোনো প্রশ্ন করতে গেলে ভালো ভাবেই কর...  \n",
       "\n",
       "[1434 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_final/Exp1/Bengali/bengali_train_pairs.csv', encoding='utf-8',lineterminator='\\n')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a294a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f8f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c6b931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(len(train_data)):\n",
    "    di = {}\n",
    "    di['input_text'] = train_data.iloc[i]['input_text']\n",
    "    di['predicted_text'] = train_data.iloc[i]['target_text']\n",
    "    d.append([di])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e928d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# out_file = open(\"temp.json\", \"w\")\n",
    "json.dump(d, open(\"temp.json\", \"w\"), indent = 4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24ab9788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/data_hindi/hindi_train.csv', encoding='utf-8')\n",
    "num_batches=int(len(data)/4)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad925b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self):\n",
    "        parser=configparser.ConfigParser()\n",
    "        parser.read('model_code/mT5/config.cfg')\n",
    "        base_path = parser['paths']['base_path']\n",
    "        print(base_path+parser['paths']['data_train_hindi'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe804f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b08819e076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-199198347449>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_code/mT5/config.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configparser' is not defined"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc70bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4fcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth',None)   #this displays the dataframe in full width\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34fdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_hindi/hindi_train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc89195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length():\n",
    "    df['word_count'] = df['input_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6bc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f567610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def processText(text):\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)\n",
    "    text = re.sub('@[^s]+','',text)\n",
    "    text = re.sub('<user>','',text)\n",
    "    text = re.sub('<url>','',text)\n",
    "    text = re.sub(r'#([^s]+)', r'1', text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>]', r'', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cfbfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "corpus_list =[]\n",
    "for i in range(len(removed)):\n",
    "    corpus_list +=removed[i]\n",
    "counter=collections.Counter(corpus_list)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53e5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9ed4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for i in range(len(df)):\n",
    "    corpus_list.append(df['input_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bc63bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b21b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = []\n",
    "for x in corpus_list:\n",
    "    removed.append(remove_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e3c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "452a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' <pad> लेकिन भारतवर्ष का *👉हिन्दू समाज <AIZSHDHFJFH> ऐसे घिनौने कृत्य को अब 98755790 नजरअंदाज नहीं कर सकता 😠और बिग बॉस जैसे 👉रंडी खाने वाले अंदाज के शो का हर लेबल विरोध करते ❌ हुए \"\"भारत सरकार\"\"🇮🇳 व \"\"सूचना प्रसारण मंत्रालय\"\" से ***** @skp आग्रह करता है कि , <url> इस कार्यक्रम को तत्काल बंद कराएं\",\"हालाँकि आप शायद सही कह रहे हैं https://www.xxx.com कि जहरीले टेलीविज़न शो को सेंसर किया जाना चाहिए, #terrorism लेकिन आपकी आलोचना करने का तरीका गलत था।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70de542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <pad> लेकिन भारतवर्ष का *👉हिन्दू समाज <AIZSHDHFJFH> ऐसे घिनौने कृत्य को अब 98755790 नजरअंदाज नहीं कर सकता 😠और बिग बॉस जैसे 👉रंडी खाने वाले अंदाज के शो का हर लेबल विरोध करते ❌ हुए \"\"भारत सरकार\"\"🇮🇳 व \"\"सूचना प्रसारण मंत्रालय\"\" से ***** @skp आग्रह करता है कि , <url> इस कार्यक्रम को तत्काल बंद कराएं\",\"हालाँकि आप शायद सही कह रहे हैं https://www.xxx.com कि जहरीले टेलीविज़न शो को सेंसर किया जाना चाहिए, #terrorism लेकिन आपकी आलोचना करने का तरीका गलत था।'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64407057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "    #text = re.sub('<user>','',text)\n",
    "    #text = re.sub('<url>','',text)\n",
    "    text = re.sub('<.*?>','',text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>!@#$%^&*]', r'', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6327e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'लेकिन भारतवर्ष का हिन्दू समाज ऐसे घिनौने कृत्य को अबनजरअंदाज नहीं कर सकता और बिग बॉस जैसे रंडी खाने वाले अंदाज के शो का हर लेबल विरोध करते  हुए भारत सरकार व सूचना प्रसारण मंत्रालय से skp आग्रह करता है कि , इस कार्यक्रम को तत्काल बंद कराएं,हालाँकि आप शायद सही कह रहे हैं कि जहरीले टेलीविज़न शो को सेंसर किया जाना चाहिए, terrorism लेकिन आपकी आलोचना करने का तरीका गलत था।'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = processText(text)\n",
    "text = remove_emojis(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73cf9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"    fsdf d34234sf    sdfsdf   \"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ef910e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"\\s+[a-z]\\s+\", \" \", text, flags = re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c7e5b382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fsdf d34234sf sdfsdf '"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f122cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = text\n",
    "string = re.sub(r\"/  +/g\", r'',string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a996f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545065e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebb2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee80a858",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b5cc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.329838Z",
     "start_time": "2022-11-12T10:25:27.482040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,recall_score,precision_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79fc2370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.360518Z",
     "start_time": "2022-11-12T10:25:28.330961Z"
    }
   },
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "\n",
    "def processText(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "    #text = re.sub('<user>','',text)\n",
    "    #text = re.sub('<url>','',text)\n",
    "    text = re.sub('<.*?>','',text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>!@#$%^&*]', r'', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    _RE_COMBINE_WHITESPACE = re.compile(r\"(?a:\\s+)\")\n",
    "    _RE_STRIP_WHITESPACE = re.compile(r\"(?a:^\\s+|\\s+$)\")\n",
    "    text = _RE_COMBINE_WHITESPACE.sub(\" \", text)\n",
    "    text = _RE_STRIP_WHITESPACE.sub(\"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_emojis(text)\n",
    "    text = processText(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b611ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.374423Z",
     "start_time": "2022-11-12T10:25:28.361672Z"
    }
   },
   "outputs": [],
   "source": [
    "def hate_refrences(data,test_set):          ###############returns pair of <hate,refrences>  \n",
    "    hate  = []\n",
    "    reply = []\n",
    "    refrences = []\n",
    "    for sample in data:\n",
    "        ht , rep = sample[0] , sample[1]\n",
    "        hate.append(ht)\n",
    "        reply.append(rep)\n",
    "    hate = list(set(hate))\n",
    "    mp={}\n",
    "    for ht_i in hate:\n",
    "        refs = []\n",
    "        for sample in data:\n",
    "            ht_j , rep =  sample[0] , sample[1]\n",
    "            if ht_j == ht_i:\n",
    "                refs.append(rep)\n",
    "        mp[ht_i] = refs\n",
    "        refrences.append(refs)\n",
    "    hate = list(set([x[0] for x in test_set]))\n",
    "    refs = [mp[ht_i] for ht_i in hate]\n",
    "    return hate,refs             # a given hate instance and refrences(replies) for metrics evaluation\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def training_corpus(train_set):    # returns training corpus\n",
    "    replies = []\n",
    "    for sample in train_set:\n",
    "        rep = sample[1]\n",
    "        replies.append(rep)\n",
    "    replies = list(set(replies))\n",
    "    return replies                # returns the sentences used while training \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(params, model, test_dataloader, device):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch[0], batch[0])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "###################################### BLEU_SCORE , METEOR #######################################\n",
    "def hate_refrences(data, test_set):          ###############returns pair of <hate,refrences>  \n",
    "    hate  = []\n",
    "    reply = []\n",
    "    refrences = []\n",
    "    for ind in data.index:\n",
    "        ht , rep = data['input_text'][ind] , data['target_text'][ind]\n",
    "        hate.append(ht)\n",
    "        reply.append(rep)\n",
    "    hate = list(set(hate))\n",
    "    mp={}\n",
    "    for ht_i in hate:\n",
    "        refs = []\n",
    "        for ind in data.index:\n",
    "            ht_j , rep =  data['input_text'][ind] , data['target_text'][ind]\n",
    "            if ht_j == ht_i:\n",
    "                refs.append(rep)\n",
    "        mp[ht_i] = refs\n",
    "        refrences.append(refs)\n",
    "    #hate = list(set([x[0] for x in test_set]))\n",
    "    #refs = [mp[ht_i] for ht_i in hate]\n",
    "    return hate, refrences   \n",
    "\n",
    "\n",
    "\n",
    "############################################ JACCARD SIMILARITY #################################\n",
    "def get_jaccard_sim(str1, str2):   \n",
    "    if isinstance(str1, float) or isinstance(str2, float):\n",
    "        return (-1)\n",
    "    try:\n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    except:\n",
    "        print((str1))\n",
    "        print(type(str2))\n",
    "        return 0\n",
    "\n",
    "\n",
    "############################################### NOVELTY #########################################\n",
    "def get_novelty(sent, training_corpus):\n",
    "    max_overlap = 0\n",
    "    for instance in training_corpus:\n",
    "        max_overlap = max(max_overlap,get_jaccard_sim(instance,sent))\n",
    "    return 1-max_overlap\n",
    "\n",
    "def avg_novelty(sentences,training_corpus):\n",
    "    avg = 0\n",
    "    for sent in sentences:\n",
    "        avg += get_novelty(sent,training_corpus)\n",
    "    avg = (avg/float(len(sentences)))\n",
    "    return avg\n",
    "\n",
    "\n",
    "\n",
    "############################################### DIVERSITY ########################################\n",
    "def get_diversity(sentences):\n",
    "    avg = 0.0\n",
    "    for i in range(len(sentences)):\n",
    "        max_overlap = 0\n",
    "        for j in range(len(sentences)):\n",
    "            if i!=j:\n",
    "                max_overlap = max(max_overlap,get_jaccard_sim(sentences[i],sentences[j]))\n",
    "        avg = avg + (1-max_overlap)\n",
    "    avg = (avg/len(sentences))\n",
    "    return avg, len(sentences)\n",
    "    \n",
    "def diversity_and_novelty(training_corpus, gen_replies):\n",
    "    diversity = get_diversity(gen_replies)\n",
    "    novelty   = 0#avg_novelty(gen_replies,training_corpus)\n",
    "    return diversity,novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28ffbf44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:36:09.734823Z",
     "start_time": "2022-11-12T10:36:09.324262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-a47f2dd78855>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['input_text'][ind] =  preprocess(df_pred['input_text'][ind])\n",
      "<ipython-input-58-a47f2dd78855>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['predicted_text'][ind] =  preprocess(df_pred['predicted_text'][ind])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_train = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp3/Exp3a/Hindi/hindi_train_pairs.csv') #,lineterminator='\\n')\n",
    "# df_pred = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/outputs/Exp3/Exp3a/bloom/bloom_hindi/counter_bloom_hindi.csv') #,lineterminator='\\n')   \n",
    "# df_test = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp3/Exp3a/Hindi/hindi_test_pairs.csv') #,lineterminator='\\n')\n",
    "\n",
    "df_train = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp1/Hindi/hindi_train_pairs.csv') #,lineterminator='\\n')\n",
    "df_pred = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/outputs/Exp3/Exp3a/bloom/bloom_hindi/counter_bloom_hindi.csv') #,lineterminator='\\n')   \n",
    "df_test = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp1/Hindi/hindi_test_pairs.csv') #,lineterminator='\\n')\n",
    "\n",
    "\n",
    "for ind in df_pred.index:\n",
    "        df_pred['input_text'][ind] =  preprocess(df_pred['input_text'][ind])\n",
    "        df_pred['predicted_text'][ind] =  preprocess(df_pred['predicted_text'][ind])\n",
    "        \n",
    "for ind in df_train.index:\n",
    "        df_train['input_text'][ind] =  preprocess(df_train['input_text'][ind])\n",
    "        df_train['target_text'][ind] =  preprocess(df_train['target_text'][ind])\n",
    "\n",
    "for ind in df_test.index:\n",
    "        df_test['input_text'][ind] =  preprocess(df_test['input_text'][ind])\n",
    "        df_test['target_text'][ind] =  preprocess(df_test['target_text'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa76bcf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:19.006753Z",
     "start_time": "2022-11-12T10:36:09.736068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Scores\n",
      "Input pred:  (0.0, 356)\n",
      "Predicted pred:  (0.33840328349286325, 356)\n",
      "Novelty Score between 1) predicted and test counters 2) predicted and train counters\n",
      "0.8105171685650658 0.5067006825692384\n"
     ]
    }
   ],
   "source": [
    "## Diversity Scores\n",
    "print(\"Diversity Scores\")\n",
    "#print(\"Input train: \", get_diversity(df_train['input_text']))\n",
    "#print(\"Target train: \", get_diversity(df_train['target_text']))\n",
    "\n",
    "#print(\"Input test: \", get_diversity(df_test['input_text']))\n",
    "#print(\"Target test: \", get_diversity(df_test['target_text']))\n",
    "\n",
    "print(\"Input pred: \", get_diversity(df_pred['input_text']))\n",
    "print(\"Predicted pred: \", get_diversity(df_pred['predicted_text']))\n",
    "\n",
    "\n",
    "## Novelty Scores\n",
    "# print(\"Novelty Scores\")\n",
    "# print(avg_novelty(df_train['input_text'], df_train['input_text']), avg_novelty(df_train['input_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_train['input_text'], df_test['input_text']), avg_novelty(df_train['input_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_train['input_text'], df_pred['input_text']), avg_novelty(df_train['input_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_train['target_text'], df_train['input_text']), avg_novelty(df_train['target_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_train['target_text'], df_test['input_text']), avg_novelty(df_train['target_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_train['target_text'], df_pred['input_text']), avg_novelty(df_train['target_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_test['input_text'], df_train['input_text']), avg_novelty(df_test['input_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_test['input_text'], df_test['input_text']), avg_novelty(df_test['input_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_test['input_text'], df_pred['input_text']), avg_novelty(df_test['input_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_test['target_text'], df_train['input_text']), avg_novelty(df_test['target_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_test['target_text'], df_test['input_text']), avg_novelty(df_test['target_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_test['target_text'], df_pred['input_text']), avg_novelty(df_test['target_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_train['input_text']), avg_novelty(df_pred['predicted_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_test['input_text']), avg_novelty(df_pred['predicted_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_pred['input_text']), avg_novelty(df_pred['predicted_text'], df_pred['predicted_text']))\n",
    "\n",
    "\n",
    "print(\"Novelty Score between 1) predicted and test counters 2) predicted and train counters\")\n",
    "print(avg_novelty(df_pred['predicted_text'], df_test['target_text']), avg_novelty(df_pred['predicted_text'], df_train['target_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "603f309f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:30.089256Z",
     "start_time": "2022-11-12T10:38:19.007938Z"
    }
   },
   "outputs": [],
   "source": [
    "## bleu and meteor scores\n",
    "hate  = []\n",
    "reply = []\n",
    "refrences = []\n",
    "for ind in df_train.index:\n",
    "    ht , rep = df_train['input_text'][ind] , df_train['target_text'][ind]\n",
    "    hate.append(ht)\n",
    "    reply.append(rep)\n",
    "\n",
    "for ind in df_test.index:\n",
    "    ht , rep = df_test['input_text'][ind] , df_test['target_text'][ind]\n",
    "    hate.append(ht)\n",
    "    reply.append(rep)\n",
    "\n",
    "hate = list(set(hate))\n",
    "mp={}\n",
    "\n",
    "for ht_i in hate:\n",
    "    refs = []\n",
    "    for ind in df_train.index:\n",
    "        ht_j , rep =  df_train['input_text'][ind] , df_train['target_text'][ind]\n",
    "        if ht_j == ht_i:\n",
    "            refs.append(rep)\n",
    "    for ind in df_test.index:\n",
    "        ht_j , rep =  df_test['input_text'][ind] , df_test['target_text'][ind]\n",
    "        if ht_j == ht_i:\n",
    "            refs.append(rep)\n",
    "    mp[ht_i] = refs\n",
    "    refrences.append(refs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bd5ef4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d584b53e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:30.093159Z",
     "start_time": "2022-11-12T10:38:30.090371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf5b7138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:31.661748Z",
     "start_time": "2022-11-12T10:38:30.094164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score  0.0003524877519895767\n",
      "Bleu -2 Score with Smoothener 0.13159485916444197\n",
      "Bleu -3 Score with Smoothener 0.09100366818332035\n",
      "Meteor Score 0.09522057224499196\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_2 =bleu_3= meteor_ = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    ref_list = []\n",
    "    for i in range(len(ref)):\n",
    "        ref_list.append(word_tokenize(ref[i]))\n",
    "    bleu += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters))\n",
    "    bleu_2  += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters), smoothing_function=SmoothingFunction().method2, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3  += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters), smoothing_function=SmoothingFunction().method2, weights=(0.33, 0.33, 0.33, 0))\n",
    "    \n",
    "    meteor_ += meteor_score(ref_list, word_tokenize(counters))\n",
    "\n",
    "bleu    /= len(df_pred)\n",
    "bleu_2  /= len(df_pred)\n",
    "bleu_3 /= len(df_pred)\n",
    "meteor_ /= len(df_pred)\n",
    "\n",
    "print(\"Bleu Score \", bleu)\n",
    "print(\"Bleu -2 Score with Smoothener\", bleu_2)\n",
    "print(\"Bleu -3 Score with Smoothener\", bleu_3)\n",
    "print(\"Meteor Score\", meteor_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d06485",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOOm: Zero-shot\n",
    "Bleu Score  0.0003524877519895767\n",
    "Bleu -2 Score with Smoothener 0.12544586389177953\n",
    "Bleu -3 Score with Smoothener 0.08804423597386618\n",
    "Meteor Score 0.0929422608818972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b50ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:32.158987Z",
     "start_time": "2022-11-12T10:38:31.662654Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'তোমার দেখতে ছেলেদের মতো। যদি তোমার দেখতে মেয়েদের মতো হতো। তাহলে তুমি অবশ্যই বেশ্যা হতে।'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-93f7c8c35260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mhates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcounters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'তোমার দেখতে ছেলেদের মতো। যদি তোমার দেখতে মেয়েদের মতো হতো। তাহলে তুমি অবশ্যই বেশ্যা হতে।'"
     ]
    }
   ],
   "source": [
    "def rec(str1, str2):\n",
    "    match = 0.0\n",
    "    tok1 = word_tokenize(str1)\n",
    "    tok2 = word_tokenize(str2)\n",
    "    if(len(tok1)==0 or len(tok2)==0):\n",
    "        return -999\n",
    "    for i in tok1:\n",
    "        for j in tok2:\n",
    "            if i == j:\n",
    "                match += 1.0\n",
    "                break;\n",
    "    return match/len(tok1)\n",
    "\n",
    "def rec2(str1, str2):\n",
    "    match = 0.0\n",
    "    tok1 = word_tokenize(str1)\n",
    "    tok2 = word_tokenize(str2)\n",
    "    for i in tok2:\n",
    "        for j in tok1:\n",
    "            if i == j:\n",
    "                match += 1.0\n",
    "                break;\n",
    "    return match/len(tok2)\n",
    "\n",
    "recall = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    recall2 = 0.0\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    for i in range(len(ref)):\n",
    "        recall2 = max(recall2, rec(counters, ref[i]))\n",
    "        #print(recall2)\n",
    "    \n",
    "    recall += recall2\n",
    "\n",
    "recall    /= len(df_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "recall = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    recall2 = 0.0\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    for i in range(len(ref)):\n",
    "        recall2 = max(recall2, rec2(counters, ref[i]))\n",
    "        #print(recall2)\n",
    "    \n",
    "    recall += recall2\n",
    "\n",
    "recall    /= len(df_pred)\n",
    "\n",
    "print(\"Precision: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf0054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad71548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4231260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67223c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

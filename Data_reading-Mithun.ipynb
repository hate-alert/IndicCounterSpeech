{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4552e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "acbb4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_train_pairs.csv',encoding='utf-8')\n",
    "# df_train_trans = pd.read_csv('data_final_0711/Exp2/Hindi/bengaliTranstoHindi.csv',encoding='utf-8')\n",
    "# df_val = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_val_pairs.csv', encoding='utf-8')\n",
    "# df_test = pd.read_csv('data_final_0711/Exp2/Hindi/hindi_test_pairs.csv', encoding='utf-8')\n",
    "\n",
    "df_train = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_train_pairs.csv',encoding='utf-8',lineterminator='\\n')\n",
    "df_train_trans = pd.read_csv('data_final_0711/Exp2/Bengali/hindiTranstoBengali.csv',encoding='utf-8')\n",
    "df_val = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_val_pairs.csv', encoding='utf-8')\n",
    "df_test = pd.read_csv('data_final_0711/Exp2/Bengali/bengali_test_pairs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640deb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def util(df_train, save_file_name):\n",
    "    df_train = df_train[['texts','counterSpeechs']]\n",
    "    df_train['prefix'] = 'counterspeech'\n",
    "    first_column = df_train.pop('prefix')\n",
    "    df_train.insert(0, 'prefix', first_column)\n",
    "    df_train = df_train.rename(columns={'texts': 'input_text', 'counterSpeechs': 'target_text'})\n",
    "    df_train.to_csv('data_final/Exp2/Bengali/'+save_file_name,encoding='utf-8',index=False)\n",
    "#     print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d5d02727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trans = df_train_trans[['hatespeech_trans','counterspeech_trans']]\n",
    "df_train_trans = df_train_trans.rename(columns = {'hatespeech_trans':'texts', 'counterspeech_trans':'counterSpeechs'})\n",
    "df_combined = df_train.append(df_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b47a2828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-79044556ebc6>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['prefix'] = 'counterspeech'\n"
     ]
    }
   ],
   "source": [
    "# util(df_combined,'hindi_train_pairs.csv')\n",
    "# util(df_val,'hindi_val_pairs.csv')\n",
    "# util(df_test,'hindi_test_pairs.csv')\n",
    "\n",
    "util(df_combined,'bengali_train_pairs.csv')\n",
    "util(df_val,'bengali_val_pairs.csv')\n",
    "util(df_test,'bengali_test_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794efe13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9f25755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ ‡¶§‡ßã‡¶ï ‡¶ö‡ßÅ‡¶á‡¶¶‡¶æ ‡¶§‡ßã‡¶∞ ‡¶≠‡ßã‡¶¶‡¶æ ...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Æ‡ßÅ‡¶ñ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶ú‡¶ò‡¶®‡ßç‡¶Ø‡•§ ‡¶ï‡ßã‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ï‡ßá ‡¶Ö‡¶≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ ‡¶§‡ßã‡¶ï ‡¶ö‡ßÅ‡¶á‡¶¶‡¶æ ‡¶§‡ßã‡¶∞ ‡¶≠‡ßã‡¶¶‡¶æ ...</td>\n",
       "      <td>‡¶≤‡ßá‡¶ñ‡¶ï ‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶è‡¶á ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶¨‡ßÅ‡¶ù‡¶ø‡ßü‡ßá ‡¶¶‡¶ø‡¶õ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶¨‡¶æ‡¶¨‡¶æ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶è‡¶ï‡¶ü‡¶æ</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶ó‡¶æ‡¶≤‡¶ø‡¶ó‡¶æ‡¶≤‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶® ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶¨‡¶≤‡ßá ‡¶Æ‡¶®‡ßá ‡¶π‡ßü‡•§ ‡¶ö‡¶æ‡¶∞‡¶ü...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶¨‡¶æ‡¶¨‡¶æ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶è‡¶ï‡¶ü‡¶æ</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶¶‡¶ø ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶ò‡ßÉ‡¶£‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡¶® ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡¶Æ‡¶æ‡¶ú ‡¶Ü‡¶™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ö‡ßã‡¶¶‡¶®‡ßá expart ! ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶§‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ú‡¶ø‡¶û...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø‡¶∏‡ßá ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡ßá‡¶â ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶®‡¶ø‡•§ ‡¶Ö‡¶≠‡¶¶‡ßç‡¶∞ ‡¶Æ‡¶®‡ßç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶∏‡¶§‡ßç‡¶Ø‡¶ø  ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞  ‡¶Æ‡¶® ‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï‡¶§‡¶æ‡¶Ø‡¶º ‡¶è‡¶ñ‡¶®‡¶ì ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡¶ø‡¶õ‡¶ø‡¶Ø‡¶º‡ßá...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡ßá ‡¶ï‡¶•‡¶æ‡¶ü‡¶ø ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡ßá‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡¶è‡¶ü‡¶æ‡¶á ‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶∏‡¶¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤ ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®,  ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â...</td>\n",
       "      <td>‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡ßü ‡¶¨‡ßá‡¶∂‡¶ø ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ, ‡¶§‡¶ñ‡¶®...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤ ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®,  ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â...</td>\n",
       "      <td>‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶∏‡¶ï‡¶≤‡¶ï‡ßá‡¶á ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶∂‡¶ï‡ßç‡¶§‡¶ø ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡ßü...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶Ü‡¶ö‡ßç‡¶õ‡¶æ ‡¶§‡ßÅ‡¶á ‡¶è‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶è‡¶§ ‡¶∞‡¶æ‡¶ó ‡¶ï‡ßá‡¶®? ‡¶∏‡ßá ‡¶ï‡¶ø ‡¶§‡ßã...</td>\n",
       "      <td>‡¶ì‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶π‡ßü‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶§‡ßá ‡¶ó‡¶ø‡ßü‡ßá ‡¶Ü‡¶™‡¶®‡¶ø‡¶ì ‡¶§‡ßã ‡¶ó‡¶æ‡¶≤‡¶ø‡¶ó‡¶æ‡¶≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>counterspeech</td>\n",
       "      <td>‡¶Ü‡¶ö‡ßç‡¶õ‡¶æ ‡¶§‡ßÅ‡¶á ‡¶è‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶è‡¶§ ‡¶∞‡¶æ‡¶ó ‡¶ï‡ßá‡¶®? ‡¶∏‡ßá ‡¶ï‡¶ø ‡¶§‡ßã...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶æ‡¶â‡¶ï‡ßá ‡¶ï‡ßã‡¶®‡ßã ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ó‡ßá‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶ï‡¶∞...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1434 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             prefix                                         input_text  \\\n",
       "0     counterspeech  ‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ ‡¶§‡ßã‡¶ï ‡¶ö‡ßÅ‡¶á‡¶¶‡¶æ ‡¶§‡ßã‡¶∞ ‡¶≠‡ßã‡¶¶‡¶æ ...   \n",
       "1     counterspeech  ‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶ï‡ßÅ‡¶§‡ßç‡¶§‡¶æ ‡¶§‡ßã‡¶ï ‡¶ö‡ßÅ‡¶á‡¶¶‡¶æ ‡¶§‡ßã‡¶∞ ‡¶≠‡ßã‡¶¶‡¶æ ...   \n",
       "2     counterspeech                           ‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶¨‡¶æ‡¶¨‡¶æ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶è‡¶ï‡¶ü‡¶æ   \n",
       "3     counterspeech                           ‡¶∂‡¶æ‡¶≤‡ßÄ ‡¶¨‡¶æ‡¶¨‡¶æ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶è‡¶ï‡¶ü‡¶æ   \n",
       "4     counterspeech  ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ö‡ßã‡¶¶‡¶®‡ßá expart ! ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶ö‡ßÅ‡¶¶‡¶ø ‡¶§‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ú‡¶ø‡¶û...   \n",
       "...             ...                                                ...   \n",
       "1429  counterspeech  ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø  ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞  ‡¶Æ‡¶® ‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï‡¶§‡¶æ‡¶Ø‡¶º ‡¶è‡¶ñ‡¶®‡¶ì ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡¶ø‡¶õ‡¶ø‡¶Ø‡¶º‡ßá...   \n",
       "1430  counterspeech  ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤ ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®,  ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â...   \n",
       "1431  counterspeech  ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤ ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®,  ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â...   \n",
       "1432  counterspeech  ‡¶Ü‡¶ö‡ßç‡¶õ‡¶æ ‡¶§‡ßÅ‡¶á ‡¶è‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶è‡¶§ ‡¶∞‡¶æ‡¶ó ‡¶ï‡ßá‡¶®? ‡¶∏‡ßá ‡¶ï‡¶ø ‡¶§‡ßã...   \n",
       "1433  counterspeech  ‡¶Ü‡¶ö‡ßç‡¶õ‡¶æ ‡¶§‡ßÅ‡¶á ‡¶è‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶è‡¶§ ‡¶∞‡¶æ‡¶ó ‡¶ï‡ßá‡¶®? ‡¶∏‡ßá ‡¶ï‡¶ø ‡¶§‡ßã...   \n",
       "\n",
       "                                            target_text  \n",
       "0     ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Æ‡ßÅ‡¶ñ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶ú‡¶ò‡¶®‡ßç‡¶Ø‡•§ ‡¶ï‡ßã‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ï‡ßá ‡¶Ö‡¶≠...  \n",
       "1     ‡¶≤‡ßá‡¶ñ‡¶ï ‡¶§‡¶æ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶è‡¶á ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶¨‡ßÅ‡¶ù‡¶ø‡ßü‡ßá ‡¶¶‡¶ø‡¶õ...  \n",
       "2     ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ó‡¶æ‡¶≤‡¶ø‡¶ó‡¶æ‡¶≤‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶® ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶¨‡¶≤‡ßá ‡¶Æ‡¶®‡ßá ‡¶π‡ßü‡•§ ‡¶ö‡¶æ‡¶∞‡¶ü...  \n",
       "3     ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶¶‡¶ø ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶ò‡ßÉ‡¶£‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡¶® ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡¶Æ‡¶æ‡¶ú ‡¶Ü‡¶™...  \n",
       "4     ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø‡¶∏‡ßá ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡¶æ‡¶∞‡ßç‡¶ü ‡¶ï‡ßá‡¶â ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶®‡¶ø‡•§ ‡¶Ö‡¶≠‡¶¶‡ßç‡¶∞ ‡¶Æ‡¶®‡ßç...  \n",
       "...                                                 ...  \n",
       "1429  ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡ßá ‡¶ï‡¶•‡¶æ‡¶ü‡¶ø ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡ßá‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡¶è‡¶ü‡¶æ‡¶á ‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶∏‡¶¨...  \n",
       "1430  ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡ßü ‡¶¨‡ßá‡¶∂‡¶ø ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ, ‡¶§‡¶ñ‡¶®...  \n",
       "1431  ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶∞‡ßÄ ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶∏‡¶ï‡¶≤‡¶ï‡ßá‡¶á ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶∂‡¶ï‡ßç‡¶§‡¶ø ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡¶æ‡¶®‡¶ø‡ßü...  \n",
       "1432  ‡¶ì‡¶á ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶π‡ßü‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶§‡ßá ‡¶ó‡¶ø‡ßü‡ßá ‡¶Ü‡¶™‡¶®‡¶ø‡¶ì ‡¶§‡ßã ‡¶ó‡¶æ‡¶≤‡¶ø‡¶ó‡¶æ‡¶≤...  \n",
       "1433  ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶æ‡¶â‡¶ï‡ßá ‡¶ï‡ßã‡¶®‡ßã ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ó‡ßá‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶ï‡¶∞...  \n",
       "\n",
       "[1434 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_final/Exp1/Bengali/bengali_train_pairs.csv', encoding='utf-8',lineterminator='\\n')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a294a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f8f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c6b931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(len(train_data)):\n",
    "    di = {}\n",
    "    di['input_text'] = train_data.iloc[i]['input_text']\n",
    "    di['predicted_text'] = train_data.iloc[i]['target_text']\n",
    "    d.append([di])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e928d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# out_file = open(\"temp.json\", \"w\")\n",
    "json.dump(d, open(\"temp.json\", \"w\"), indent = 4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24ab9788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/data_hindi/hindi_train.csv', encoding='utf-8')\n",
    "num_batches=int(len(data)/4)\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad925b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self):\n",
    "        parser=configparser.ConfigParser()\n",
    "        parser.read('model_code/mT5/config.cfg')\n",
    "        base_path = parser['paths']['base_path']\n",
    "        print(base_path+parser['paths']['data_train_hindi'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe804f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b08819e076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-199198347449>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_code/mT5/config.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configparser' is not defined"
     ]
    }
   ],
   "source": [
    "Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc70bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4fcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth',None)   #this displays the dataframe in full width\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34fdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_hindi/hindi_train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc89195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_length():\n",
    "    df['word_count'] = df['input_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6bc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df94f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f567610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def processText(text):\n",
    "    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)\n",
    "    text = re.sub('@[^s]+','',text)\n",
    "    text = re.sub('<user>','',text)\n",
    "    text = re.sub('<url>','',text)\n",
    "    text = re.sub(r'#([^s]+)', r'1', text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>]', r'', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cfbfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "corpus_list =[]\n",
    "for i in range(len(removed)):\n",
    "    corpus_list +=removed[i]\n",
    "counter=collections.Counter(corpus_list)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53e5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb9ed4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for i in range(len(df)):\n",
    "    corpus_list.append(df['input_text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bc63bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b21b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = []\n",
    "for x in corpus_list:\n",
    "    removed.append(remove_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e3c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "452a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' <pad> ‡§≤‡•á‡§ï‡§ø‡§® ‡§≠‡§æ‡§∞‡§§‡§µ‡§∞‡•ç‡§∑ ‡§ï‡§æ *üëâ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§∏‡§Æ‡§æ‡§ú <AIZSHDHFJFH> ‡§ê‡§∏‡•á ‡§ò‡§ø‡§®‡•å‡§®‡•á ‡§ï‡•É‡§§‡•ç‡§Ø ‡§ï‡•ã ‡§Ö‡§¨ 98755790 ‡§®‡§ú‡§∞‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ üò†‡§î‡§∞ ‡§¨‡§ø‡§ó ‡§¨‡•â‡§∏ ‡§ú‡•à‡§∏‡•á üëâ‡§∞‡§Ç‡§°‡•Ä ‡§ñ‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§ï‡•á ‡§∂‡•ã ‡§ï‡§æ ‡§π‡§∞ ‡§≤‡•á‡§¨‡§≤ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§§‡•á ‚ùå ‡§π‡•Å‡§è \"\"‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞\"\"üáÆüá≥ ‡§µ \"\"‡§∏‡•Ç‡§ö‡§®‡§æ ‡§™‡•ç‡§∞‡§∏‡§æ‡§∞‡§£ ‡§Æ‡§Ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø\"\" ‡§∏‡•á ***** @skp ‡§Ü‡§ó‡•ç‡§∞‡§π ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø , <url> ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§ï‡•ã ‡§§‡§§‡•ç‡§ï‡§æ‡§≤ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡§æ‡§è‡§Ç\",\"‡§π‡§æ‡§≤‡§æ‡§Å‡§ï‡§ø ‡§Ü‡§™ ‡§∂‡§æ‡§Ø‡§¶ ‡§∏‡§π‡•Ä ‡§ï‡§π ‡§∞‡§π‡•á ‡§π‡•à‡§Ç https://www.xxx.com ‡§ï‡§ø ‡§ú‡§π‡§∞‡•Ä‡§≤‡•á ‡§ü‡•á‡§≤‡•Ä‡§µ‡§ø‡§ú‡§º‡§® ‡§∂‡•ã ‡§ï‡•ã ‡§∏‡•á‡§Ç‡§∏‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, #terrorism ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§§‡§∞‡•Ä‡§ï‡§æ ‡§ó‡§≤‡§§ ‡§•‡§æ‡•§'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70de542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <pad> ‡§≤‡•á‡§ï‡§ø‡§® ‡§≠‡§æ‡§∞‡§§‡§µ‡§∞‡•ç‡§∑ ‡§ï‡§æ *üëâ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§∏‡§Æ‡§æ‡§ú <AIZSHDHFJFH> ‡§ê‡§∏‡•á ‡§ò‡§ø‡§®‡•å‡§®‡•á ‡§ï‡•É‡§§‡•ç‡§Ø ‡§ï‡•ã ‡§Ö‡§¨ 98755790 ‡§®‡§ú‡§∞‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ üò†‡§î‡§∞ ‡§¨‡§ø‡§ó ‡§¨‡•â‡§∏ ‡§ú‡•à‡§∏‡•á üëâ‡§∞‡§Ç‡§°‡•Ä ‡§ñ‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§ï‡•á ‡§∂‡•ã ‡§ï‡§æ ‡§π‡§∞ ‡§≤‡•á‡§¨‡§≤ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§§‡•á ‚ùå ‡§π‡•Å‡§è \"\"‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞\"\"üáÆüá≥ ‡§µ \"\"‡§∏‡•Ç‡§ö‡§®‡§æ ‡§™‡•ç‡§∞‡§∏‡§æ‡§∞‡§£ ‡§Æ‡§Ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø\"\" ‡§∏‡•á ***** @skp ‡§Ü‡§ó‡•ç‡§∞‡§π ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø , <url> ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§ï‡•ã ‡§§‡§§‡•ç‡§ï‡§æ‡§≤ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡§æ‡§è‡§Ç\",\"‡§π‡§æ‡§≤‡§æ‡§Å‡§ï‡§ø ‡§Ü‡§™ ‡§∂‡§æ‡§Ø‡§¶ ‡§∏‡§π‡•Ä ‡§ï‡§π ‡§∞‡§π‡•á ‡§π‡•à‡§Ç https://www.xxx.com ‡§ï‡§ø ‡§ú‡§π‡§∞‡•Ä‡§≤‡•á ‡§ü‡•á‡§≤‡•Ä‡§µ‡§ø‡§ú‡§º‡§® ‡§∂‡•ã ‡§ï‡•ã ‡§∏‡•á‡§Ç‡§∏‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, #terrorism ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§§‡§∞‡•Ä‡§ï‡§æ ‡§ó‡§≤‡§§ ‡§•‡§æ‡•§'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64407057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "    #text = re.sub('<user>','',text)\n",
    "    #text = re.sub('<url>','',text)\n",
    "    text = re.sub('<.*?>','',text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>!@#$%^&*]', r'', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6327e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§≤‡•á‡§ï‡§ø‡§® ‡§≠‡§æ‡§∞‡§§‡§µ‡§∞‡•ç‡§∑ ‡§ï‡§æ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç ‡§∏‡§Æ‡§æ‡§ú ‡§ê‡§∏‡•á ‡§ò‡§ø‡§®‡•å‡§®‡•á ‡§ï‡•É‡§§‡•ç‡§Ø ‡§ï‡•ã ‡§Ö‡§¨‡§®‡§ú‡§∞‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§î‡§∞ ‡§¨‡§ø‡§ó ‡§¨‡•â‡§∏ ‡§ú‡•à‡§∏‡•á ‡§∞‡§Ç‡§°‡•Ä ‡§ñ‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§Ö‡§Ç‡§¶‡§æ‡§ú ‡§ï‡•á ‡§∂‡•ã ‡§ï‡§æ ‡§π‡§∞ ‡§≤‡•á‡§¨‡§≤ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§§‡•á  ‡§π‡•Å‡§è ‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§µ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§™‡•ç‡§∞‡§∏‡§æ‡§∞‡§£ ‡§Æ‡§Ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø ‡§∏‡•á skp ‡§Ü‡§ó‡•ç‡§∞‡§π ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø , ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§ï‡•ã ‡§§‡§§‡•ç‡§ï‡§æ‡§≤ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡§æ‡§è‡§Ç,‡§π‡§æ‡§≤‡§æ‡§Å‡§ï‡§ø ‡§Ü‡§™ ‡§∂‡§æ‡§Ø‡§¶ ‡§∏‡§π‡•Ä ‡§ï‡§π ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ú‡§π‡§∞‡•Ä‡§≤‡•á ‡§ü‡•á‡§≤‡•Ä‡§µ‡§ø‡§ú‡§º‡§® ‡§∂‡•ã ‡§ï‡•ã ‡§∏‡•á‡§Ç‡§∏‡§∞ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, terrorism ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§™‡§ï‡•Ä ‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§§‡§∞‡•Ä‡§ï‡§æ ‡§ó‡§≤‡§§ ‡§•‡§æ‡•§'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = processText(text)\n",
    "text = remove_emojis(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73cf9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"    fsdf d34234sf    sdfsdf   \"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ef910e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"\\s+[a-z]\\s+\", \" \", text, flags = re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c7e5b382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fsdf d34234sf sdfsdf '"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f122cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = text\n",
    "string = re.sub(r\"/  +/g\", r'',string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a996f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545065e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebb2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374429b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee80a858",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b5cc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.329838Z",
     "start_time": "2022-11-12T10:25:27.482040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,recall_score,precision_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79fc2370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.360518Z",
     "start_time": "2022-11-12T10:25:28.330961Z"
    }
   },
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io\n",
    "\n",
    "def processText(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "    #text = re.sub('<user>','',text)\n",
    "    #text = re.sub('<url>','',text)\n",
    "    text = re.sub('<.*?>','',text)\n",
    "    text = re.sub(r'[.!\"\\/<\\*>!@#$%^&*]', r'', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    _RE_COMBINE_WHITESPACE = re.compile(r\"(?a:\\s+)\")\n",
    "    _RE_STRIP_WHITESPACE = re.compile(r\"(?a:^\\s+|\\s+$)\")\n",
    "    text = _RE_COMBINE_WHITESPACE.sub(\" \", text)\n",
    "    text = _RE_STRIP_WHITESPACE.sub(\"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_emojis(text)\n",
    "    text = processText(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b611ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:25:28.374423Z",
     "start_time": "2022-11-12T10:25:28.361672Z"
    }
   },
   "outputs": [],
   "source": [
    "def hate_refrences(data,test_set):          ###############returns pair of <hate,refrences>  \n",
    "    hate  = []\n",
    "    reply = []\n",
    "    refrences = []\n",
    "    for sample in data:\n",
    "        ht , rep = sample[0] , sample[1]\n",
    "        hate.append(ht)\n",
    "        reply.append(rep)\n",
    "    hate = list(set(hate))\n",
    "    mp={}\n",
    "    for ht_i in hate:\n",
    "        refs = []\n",
    "        for sample in data:\n",
    "            ht_j , rep =  sample[0] , sample[1]\n",
    "            if ht_j == ht_i:\n",
    "                refs.append(rep)\n",
    "        mp[ht_i] = refs\n",
    "        refrences.append(refs)\n",
    "    hate = list(set([x[0] for x in test_set]))\n",
    "    refs = [mp[ht_i] for ht_i in hate]\n",
    "    return hate,refs             # a given hate instance and refrences(replies) for metrics evaluation\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def training_corpus(train_set):    # returns training corpus\n",
    "    replies = []\n",
    "    for sample in train_set:\n",
    "        rep = sample[1]\n",
    "        replies.append(rep)\n",
    "    replies = list(set(replies))\n",
    "    return replies                # returns the sentences used while training \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(params, model, test_dataloader, device):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch[0], batch[0])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "###################################### BLEU_SCORE , METEOR #######################################\n",
    "def hate_refrences(data, test_set):          ###############returns pair of <hate,refrences>  \n",
    "    hate  = []\n",
    "    reply = []\n",
    "    refrences = []\n",
    "    for ind in data.index:\n",
    "        ht , rep = data['input_text'][ind] , data['target_text'][ind]\n",
    "        hate.append(ht)\n",
    "        reply.append(rep)\n",
    "    hate = list(set(hate))\n",
    "    mp={}\n",
    "    for ht_i in hate:\n",
    "        refs = []\n",
    "        for ind in data.index:\n",
    "            ht_j , rep =  data['input_text'][ind] , data['target_text'][ind]\n",
    "            if ht_j == ht_i:\n",
    "                refs.append(rep)\n",
    "        mp[ht_i] = refs\n",
    "        refrences.append(refs)\n",
    "    #hate = list(set([x[0] for x in test_set]))\n",
    "    #refs = [mp[ht_i] for ht_i in hate]\n",
    "    return hate, refrences   \n",
    "\n",
    "\n",
    "\n",
    "############################################ JACCARD SIMILARITY #################################\n",
    "def get_jaccard_sim(str1, str2):   \n",
    "    if isinstance(str1, float) or isinstance(str2, float):\n",
    "        return (-1)\n",
    "    try:\n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    except:\n",
    "        print((str1))\n",
    "        print(type(str2))\n",
    "        return 0\n",
    "\n",
    "\n",
    "############################################### NOVELTY #########################################\n",
    "def get_novelty(sent, training_corpus):\n",
    "    max_overlap = 0\n",
    "    for instance in training_corpus:\n",
    "        max_overlap = max(max_overlap,get_jaccard_sim(instance,sent))\n",
    "    return 1-max_overlap\n",
    "\n",
    "def avg_novelty(sentences,training_corpus):\n",
    "    avg = 0\n",
    "    for sent in sentences:\n",
    "        avg += get_novelty(sent,training_corpus)\n",
    "    avg = (avg/float(len(sentences)))\n",
    "    return avg\n",
    "\n",
    "\n",
    "\n",
    "############################################### DIVERSITY ########################################\n",
    "def get_diversity(sentences):\n",
    "    avg = 0.0\n",
    "    for i in range(len(sentences)):\n",
    "        max_overlap = 0\n",
    "        for j in range(len(sentences)):\n",
    "            if i!=j:\n",
    "                max_overlap = max(max_overlap,get_jaccard_sim(sentences[i],sentences[j]))\n",
    "        avg = avg + (1-max_overlap)\n",
    "    avg = (avg/len(sentences))\n",
    "    return avg, len(sentences)\n",
    "    \n",
    "def diversity_and_novelty(training_corpus, gen_replies):\n",
    "    diversity = get_diversity(gen_replies)\n",
    "    novelty   = 0#avg_novelty(gen_replies,training_corpus)\n",
    "    return diversity,novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28ffbf44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:36:09.734823Z",
     "start_time": "2022-11-12T10:36:09.324262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-a47f2dd78855>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['input_text'][ind] =  preprocess(df_pred['input_text'][ind])\n",
      "<ipython-input-58-a47f2dd78855>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pred['predicted_text'][ind] =  preprocess(df_pred['predicted_text'][ind])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_train = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp3/Exp3a/Hindi/hindi_train_pairs.csv') #,lineterminator='\\n')\n",
    "# df_pred = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/outputs/Exp3/Exp3a/bloom/bloom_hindi/counter_bloom_hindi.csv') #,lineterminator='\\n')   \n",
    "# df_test = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp3/Exp3a/Hindi/hindi_test_pairs.csv') #,lineterminator='\\n')\n",
    "\n",
    "df_train = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp1/Hindi/hindi_train_pairs.csv') #,lineterminator='\\n')\n",
    "df_pred = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/outputs/Exp3/Exp3a/bloom/bloom_hindi/counter_bloom_hindi.csv') #,lineterminator='\\n')   \n",
    "df_test = pd.read_csv('/home/mithun-binny/HateAlert_Folder/JointDir/Saurabh/data_final/Exp1/Hindi/hindi_test_pairs.csv') #,lineterminator='\\n')\n",
    "\n",
    "\n",
    "for ind in df_pred.index:\n",
    "        df_pred['input_text'][ind] =  preprocess(df_pred['input_text'][ind])\n",
    "        df_pred['predicted_text'][ind] =  preprocess(df_pred['predicted_text'][ind])\n",
    "        \n",
    "for ind in df_train.index:\n",
    "        df_train['input_text'][ind] =  preprocess(df_train['input_text'][ind])\n",
    "        df_train['target_text'][ind] =  preprocess(df_train['target_text'][ind])\n",
    "\n",
    "for ind in df_test.index:\n",
    "        df_test['input_text'][ind] =  preprocess(df_test['input_text'][ind])\n",
    "        df_test['target_text'][ind] =  preprocess(df_test['target_text'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa76bcf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:19.006753Z",
     "start_time": "2022-11-12T10:36:09.736068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Scores\n",
      "Input pred:  (0.0, 356)\n",
      "Predicted pred:  (0.33840328349286325, 356)\n",
      "Novelty Score between 1) predicted and test counters 2) predicted and train counters\n",
      "0.8105171685650658 0.5067006825692384\n"
     ]
    }
   ],
   "source": [
    "## Diversity Scores\n",
    "print(\"Diversity Scores\")\n",
    "#print(\"Input train: \", get_diversity(df_train['input_text']))\n",
    "#print(\"Target train: \", get_diversity(df_train['target_text']))\n",
    "\n",
    "#print(\"Input test: \", get_diversity(df_test['input_text']))\n",
    "#print(\"Target test: \", get_diversity(df_test['target_text']))\n",
    "\n",
    "print(\"Input pred: \", get_diversity(df_pred['input_text']))\n",
    "print(\"Predicted pred: \", get_diversity(df_pred['predicted_text']))\n",
    "\n",
    "\n",
    "## Novelty Scores\n",
    "# print(\"Novelty Scores\")\n",
    "# print(avg_novelty(df_train['input_text'], df_train['input_text']), avg_novelty(df_train['input_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_train['input_text'], df_test['input_text']), avg_novelty(df_train['input_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_train['input_text'], df_pred['input_text']), avg_novelty(df_train['input_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_train['target_text'], df_train['input_text']), avg_novelty(df_train['target_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_train['target_text'], df_test['input_text']), avg_novelty(df_train['target_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_train['target_text'], df_pred['input_text']), avg_novelty(df_train['target_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_test['input_text'], df_train['input_text']), avg_novelty(df_test['input_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_test['input_text'], df_test['input_text']), avg_novelty(df_test['input_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_test['input_text'], df_pred['input_text']), avg_novelty(df_test['input_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_test['target_text'], df_train['input_text']), avg_novelty(df_test['target_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_test['target_text'], df_test['input_text']), avg_novelty(df_test['target_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_test['target_text'], df_pred['input_text']), avg_novelty(df_test['target_text'], df_pred['predicted_text']))\n",
    "\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_train['input_text']), avg_novelty(df_pred['predicted_text'], df_train['target_text']))\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_test['input_text']), avg_novelty(df_pred['predicted_text'], df_test['target_text']))\n",
    "# print(avg_novelty(df_pred['predicted_text'], df_pred['input_text']), avg_novelty(df_pred['predicted_text'], df_pred['predicted_text']))\n",
    "\n",
    "\n",
    "print(\"Novelty Score between 1) predicted and test counters 2) predicted and train counters\")\n",
    "print(avg_novelty(df_pred['predicted_text'], df_test['target_text']), avg_novelty(df_pred['predicted_text'], df_train['target_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "603f309f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:30.089256Z",
     "start_time": "2022-11-12T10:38:19.007938Z"
    }
   },
   "outputs": [],
   "source": [
    "## bleu and meteor scores\n",
    "hate  = []\n",
    "reply = []\n",
    "refrences = []\n",
    "for ind in df_train.index:\n",
    "    ht , rep = df_train['input_text'][ind] , df_train['target_text'][ind]\n",
    "    hate.append(ht)\n",
    "    reply.append(rep)\n",
    "\n",
    "for ind in df_test.index:\n",
    "    ht , rep = df_test['input_text'][ind] , df_test['target_text'][ind]\n",
    "    hate.append(ht)\n",
    "    reply.append(rep)\n",
    "\n",
    "hate = list(set(hate))\n",
    "mp={}\n",
    "\n",
    "for ht_i in hate:\n",
    "    refs = []\n",
    "    for ind in df_train.index:\n",
    "        ht_j , rep =  df_train['input_text'][ind] , df_train['target_text'][ind]\n",
    "        if ht_j == ht_i:\n",
    "            refs.append(rep)\n",
    "    for ind in df_test.index:\n",
    "        ht_j , rep =  df_test['input_text'][ind] , df_test['target_text'][ind]\n",
    "        if ht_j == ht_i:\n",
    "            refs.append(rep)\n",
    "    mp[ht_i] = refs\n",
    "    refrences.append(refs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bd5ef4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d584b53e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:30.093159Z",
     "start_time": "2022-11-12T10:38:30.090371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mithun-\n",
      "[nltk_data]     binny/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf5b7138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:31.661748Z",
     "start_time": "2022-11-12T10:38:30.094164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score  0.0003524877519895767\n",
      "Bleu -2 Score with Smoothener 0.13159485916444197\n",
      "Bleu -3 Score with Smoothener 0.09100366818332035\n",
      "Meteor Score 0.09522057224499196\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_2 =bleu_3= meteor_ = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    ref_list = []\n",
    "    for i in range(len(ref)):\n",
    "        ref_list.append(word_tokenize(ref[i]))\n",
    "    bleu += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters))\n",
    "    bleu_2  += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters), smoothing_function=SmoothingFunction().method2, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3  += nltk.translate.bleu_score.sentence_bleu(ref_list, word_tokenize(counters), smoothing_function=SmoothingFunction().method2, weights=(0.33, 0.33, 0.33, 0))\n",
    "    \n",
    "    meteor_ += meteor_score(ref_list, word_tokenize(counters))\n",
    "\n",
    "bleu    /= len(df_pred)\n",
    "bleu_2  /= len(df_pred)\n",
    "bleu_3 /= len(df_pred)\n",
    "meteor_ /= len(df_pred)\n",
    "\n",
    "print(\"Bleu Score \", bleu)\n",
    "print(\"Bleu -2 Score with Smoothener\", bleu_2)\n",
    "print(\"Bleu -3 Score with Smoothener\", bleu_3)\n",
    "print(\"Meteor Score\", meteor_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d06485",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOOm: Zero-shot\n",
    "Bleu Score  0.0003524877519895767\n",
    "Bleu -2 Score with Smoothener 0.12544586389177953\n",
    "Bleu -3 Score with Smoothener 0.08804423597386618\n",
    "Meteor Score 0.0929422608818972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b50ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T10:38:32.158987Z",
     "start_time": "2022-11-12T10:38:31.662654Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶õ‡ßá‡¶≤‡ßá‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶π‡¶§‡ßã‡•§ ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶¨‡ßá‡¶∂‡ßç‡¶Ø‡¶æ ‡¶π‡¶§‡ßá‡•§'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-93f7c8c35260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mhates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcounters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶õ‡ßá‡¶≤‡ßá‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶π‡¶§‡ßã‡•§ ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶¨‡ßá‡¶∂‡ßç‡¶Ø‡¶æ ‡¶π‡¶§‡ßá‡•§'"
     ]
    }
   ],
   "source": [
    "def rec(str1, str2):\n",
    "    match = 0.0\n",
    "    tok1 = word_tokenize(str1)\n",
    "    tok2 = word_tokenize(str2)\n",
    "    if(len(tok1)==0 or len(tok2)==0):\n",
    "        return -999\n",
    "    for i in tok1:\n",
    "        for j in tok2:\n",
    "            if i == j:\n",
    "                match += 1.0\n",
    "                break;\n",
    "    return match/len(tok1)\n",
    "\n",
    "def rec2(str1, str2):\n",
    "    match = 0.0\n",
    "    tok1 = word_tokenize(str1)\n",
    "    tok2 = word_tokenize(str2)\n",
    "    for i in tok2:\n",
    "        for j in tok1:\n",
    "            if i == j:\n",
    "                match += 1.0\n",
    "                break;\n",
    "    return match/len(tok2)\n",
    "\n",
    "recall = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    recall2 = 0.0\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    for i in range(len(ref)):\n",
    "        recall2 = max(recall2, rec(counters, ref[i]))\n",
    "        #print(recall2)\n",
    "    \n",
    "    recall += recall2\n",
    "\n",
    "recall    /= len(df_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "recall = 0.0\n",
    "\n",
    "for ind in df_pred.index:\n",
    "    recall2 = 0.0\n",
    "    hates = df_pred['input_text'][ind]\n",
    "    counters = df_pred['predicted_text'][ind]\n",
    "    ref = mp[hates]\n",
    "\n",
    "    for i in range(len(ref)):\n",
    "        recall2 = max(recall2, rec2(counters, ref[i]))\n",
    "        #print(recall2)\n",
    "    \n",
    "    recall += recall2\n",
    "\n",
    "recall    /= len(df_pred)\n",
    "\n",
    "print(\"Precision: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf0054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad71548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4231260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67223c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
